{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from LX2_masterscan import mz_ml_paths, spectra_2_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('optoins.pkl','rb') as f: options = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mzmls = mz_ml_paths(options)\n",
    "samples = [p.stem for p in mzmls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_dfs = spectra_2_df(options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spectra_dfs[0] # first file, already teim range and mass range filtered\n",
    "ms1_peaks = df.loc[df.precursor_id.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stem.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_linear_alignment(masses, options):\n",
    "    # TODO assert masses are ordered\n",
    "    up_to = None\n",
    "    for _, mass in masses.iteritems():\n",
    "        if up_to is None:\n",
    "            #up_to = mass + tolerance.getTinDA(mass) this is how its done in some places, but reuslt are not identical to below\n",
    "            up_to = mass + mass / options[\"MSresolution\"].tolerance \n",
    "        if mass <= up_to:\n",
    "            yield up_to\n",
    "        else:\n",
    "            up_to = mass + mass / options[\"MSresolution\"].tolerance\n",
    "            yield up_to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms1_peaks_agg(ms1_peaks, options):\n",
    "    ms1_peaks.sort_values(\"mz\", inplace=True)\n",
    "\n",
    "    # binning is done 3 times in lx1, between each fadi filter is performed, we do it at the end intead\n",
    "    bins1 = list(bin_linear_alignment(ms1_peaks.mz, options))\n",
    "    bins2 = list(bin_linear_alignment(ms1_peaks.groupby(bins1)[\"mz\"].transform(\"mean\"), options))\n",
    "    bins3 = list(bin_linear_alignment(ms1_peaks.groupby(bins2)[\"mz\"].transform(\"mean\"),options))\n",
    "\n",
    "    ms1_peaks['bin_mass'] = bins3 \n",
    "\n",
    "    # merge mutiple peaks from single scan\n",
    "    g = ms1_peaks.groupby([\"bin_mass\", \"scan_id\"])\n",
    "    ms1_peaks[\"scan_cumcount\"] = g.cumcount()\n",
    "    ms1_peaks[\"merged_mass\"] = g[\"mz\"].transform(\"mean\")\n",
    "    ms1_peaks['merged_inty'] = g['inty'].transform(\"mean\") # NOTE merge is NOT weighted average \n",
    "    \n",
    "\n",
    "    # aggregate results\n",
    "    agg_df = (\n",
    "        ms1_peaks.loc[ms1_peaks.scan_cumcount == 0] # use only the first of merged masses\n",
    "        .assign(\n",
    "            mass_intensity=lambda x: x.mz * x.merged_inty\n",
    "        )  # for the weighted average intensity\n",
    "        .groupby(\"bin_mass\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"merged_mass\": [\"mean\", \"count\"],\n",
    "                \"merged_inty\": [\"mean\", \"sum\"],\n",
    "                \"mass_intensity\": \"sum\",\n",
    "            }\n",
    "        )\n",
    "        .dropna()\n",
    "    )\n",
    "    agg_df.columns = [\"_\".join(col).strip() for col in agg_df.columns.values]\n",
    "\n",
    "    # apply fadi filters, in lx1 its done between each bin  process\n",
    "    fadi_denominator = ms1_peaks.scan_id.unique().shape[0]\n",
    "    mask_ff = agg_df.merged_mass_count / fadi_denominator >= options['MSfilter']\n",
    "    agg_df = agg_df[mask_ff]\n",
    "\n",
    "    # NOTE intensity threshold is do in add_Sample... but lets do it here\n",
    "    MSthreshold  = options[\"MSthreshold\"]\n",
    "    mask_inty = agg_df.merged_inty_mean > MSthreshold\n",
    "    agg_df = agg_df[mask_inty]\n",
    "\n",
    "    # for reference...weigted_mass shoud not be necesary\n",
    "    agg_df[\"weigted_mass\"] = agg_df.mass_intensity_sum / agg_df.merged_inty_sum\n",
    "    # lx1 intensity is wrong because it uses the total number of scans, instead of the numebr of scans with a peak\n",
    "    agg_df['lx1_bad_inty'] = agg_df.merged_inty_sum / fadi_denominator\n",
    "\n",
    "    \n",
    "    agg_df.rename(columns = {'merged_mass_mean':'mz', 'merged_inty_mean':'inty'}, inplace=True)\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_dfs[0].polarity.iat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for df in spectra_dfs: # first file, already teim range and mass range filtered\n",
    "    ms1_peaks = df.loc[df.precursor_id.isna()]\n",
    "    agg_df = ms1_peaks_agg(ms1_peaks, options)\n",
    "    agg_df['stem'] = df.stem.iloc[0]\n",
    "    res[df.stem.iloc[0]] = agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recalibrate\n",
    "def recalibrate_mzs(mzs, cals):\n",
    "    # from lx2_masterscan\n",
    "    # lx1 takes all that are within tolerance and then uses highest intensity\n",
    "    if not cals or mzs.empty:\n",
    "        return mzs\n",
    "    cal_matchs = [mzs.loc[mzs.sub(cal).abs().idxmin()] for cal in cals]\n",
    "    \n",
    "    cal_vals = [cal - cal_match for cal, cal_match in zip(cals, cal_matchs)]\n",
    "    # prefilter\n",
    "    if not any((v < 0.1 for v in cal_vals)):\n",
    "        return mzs\n",
    "    # find near tolerance\n",
    "    cutoff = mzs.diff(-1).quantile(0.1)\n",
    "    is_near = [v < cutoff for v in cal_vals]\n",
    "    if not any(is_near):\n",
    "        log.info(\"no valid calibration masses found\")\n",
    "        return mzs\n",
    "\n",
    "    cal_matchs = [e for e, v in zip(cal_matchs, is_near) if v]\n",
    "    cal_vals = [e for e, v in zip(cal_vals, is_near) if v]\n",
    "    log.debug(\"recalibration info: {'\\n'.join(zip(cal_matchs,cal_vals ))}\")\n",
    "\n",
    "    return mzs + np.interp(mzs, cal_matchs, cal_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#align ms1 \n",
    "ms1_agg_peaks = pd.concat(res.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1_agg_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one uses delta res for clustering\n",
    "def bin_mkSurveyLinear(masses, options):\n",
    "    # TODO assert masses are ordered\n",
    "    minmass = masses.iloc[0]\n",
    "    up_to = None\n",
    "    for _, mass in masses.iteritems():\n",
    "        if up_to is None:\n",
    "            #up_to = mass + tolerance.getTinDA(mass) this is how its done in some places, but reuslt are not identical to below\n",
    "            deltatol = options[\"MSresolution\"].tolerance + (mass - minmass) * options['MSresolutionDelta']\n",
    "            up_to = mass + (mass / deltatol)\n",
    "        if mass <= up_to:\n",
    "            yield up_to\n",
    "        else:\n",
    "            deltatol = options[\"MSresolution\"].tolerance + (mass - minmass) * options['MSresolutionDelta']\n",
    "            up_to = mass + (mass / deltatol)\n",
    "            yield up_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the method here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1_agg_peaks.sort_values('mz', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning is done 3 times in lx1, between each fadi filter is performed, we do it at the end intead\n",
    "bins1 = list(bin_mkSurveyLinear(ms1_agg_peaks.mz, options))\n",
    "bins2 = list(bin_mkSurveyLinear(ms1_agg_peaks.groupby(bins1)[\"mz\"].transform(\"mean\"), options))\n",
    "bins3 = list(bin_mkSurveyLinear(ms1_agg_peaks.groupby(bins2)[\"mz\"].transform(\"mean\"),options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1_agg_peaks['bins'] = bins3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check occupation spectracontainer.py masterscan.chekoccupation\n",
    "# occupation is the % of peak intensities abvove \"thrsld: \"\n",
    "threshold_denominator = ms1_agg_peaks.stem.unique().shape[0] # same as len(res)\n",
    "threshold = options[\"MSminOccupation\"]\n",
    "bin_peak_count = ms1_agg_peaks.groupby('bins')['inty'].transform('count')\n",
    "tf_mask = (bin_peak_count / threshold_denominator) >= threshold\n",
    "ms1_agg_peaks['above_threshold'] = tf_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1_agg_peaks['mass'] = ms1_agg_peaks.groupby('bins')['mz'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1_agg_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1_agg_peaks.pivot(index='mass', columns='stem', values=['inty','lx1_bad_inty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO collape_join_adjecent_clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS2 \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options[\"MSMSresolution\"]\n",
    "options[\"selectionWindow\"]\n",
    "\n",
    "#tolerance = TypeTolerance(\"Da\", scan.options[\"selectionWindow\"])\n",
    "#window = scan.options[\"selectionWindow\"] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spectra_dfs[1] # first file, already teim range and mass range filtered\n",
    "ms2_peaks = pd.concat((df.loc[~df.precursor_id.isna()] for df in spectra_dfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ms2_peaks[ms2_peaks.scan_id == 'controllerType=0 controllerNumber=1 scan=41']  \n",
    "*** NOTE masses are not exactly the same as in lx1, they are wrong base om raw ***  \n",
    "see xcel file on desktop  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** lx1 does the precursor binning all files, at the same time, it is not like with the ms1s ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_linear_alignment_for_ms2(masses, telerance_da):\n",
    "    # TODO assert masses are ordered\n",
    "    up_to = None\n",
    "    for _, mass in masses.iteritems():\n",
    "        if up_to is None:\n",
    "            #up_to = mass + tolerance.getTinDA(mass) this is how its done in some places, but reuslt are not identical to below\n",
    "            up_to = mass + telerance_da\n",
    "        if mass <= up_to:\n",
    "            yield up_to\n",
    "        else:\n",
    "            up_to = mass + telerance_da\n",
    "            yield up_to\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_precursors_df(ms2_peaks):\n",
    "    ms2_peaks.sort_values(['precursor_mz','mz'], inplace=True)\n",
    "    precursors_df = ms2_peaks[[\"stem\", 'scan_id', 'precursor_mz']].drop_duplicates()# similar to unqie but return a series instead of an array\n",
    "    # using \"stem\", 'scan_id' to replicate the numebr of instances for the averaging later\n",
    "    \n",
    "    bins1 = list(bin_linear_alignment_for_ms2(precursors_df.precursor_mz, options[\"selectionWindow\"]))\n",
    "    bins2 = list(bin_linear_alignment_for_ms2(precursors_df.groupby(bins1)['precursor_mz'].transform('mean'), options[\"selectionWindow\"]))\n",
    "    bins3 = list(bin_linear_alignment_for_ms2(precursors_df.groupby(bins2)['precursor_mz'].transform('mean'), options[\"selectionWindow\"]))\n",
    "\n",
    "    precursors_df['prec_bin'] = bins3\n",
    "\n",
    "# no fadi filter for precursors  \n",
    "# after groupinbg the precursors, the groups are split on the \"sample\" ie the file,  \n",
    "# the each split of the group is merged in the __def mergeListsMsms__ method that uses linear alignment  \n",
    "# thats why... prec_ms2_peaks = ms2_peaks[(ms2_peaks.precursor_mz == t) & (ms2_peaks.stem == '190321_Serum_Lipidextract_368723_01')]\n",
    "    return precursors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precursors_df = grouped_precursors_df(ms2_peaks)\n",
    "precursors_bins = precursors_df.set_index('precursor_mz')['prec_bin'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precursors_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in def linearAlignment the var clusterToMerge, it only takes the \"sample\" if its not in the cluster, so no duplicate, and only the first \"sample\"\n",
    "\n",
    "for clutering ms2 precursors, it does the binning with linearAlignment... see above, its not the unique precusrsor, its each precursor in each file,  but it does the binning on both at the same time, in contrast to how its done for ms1 binning, where first the binning is done for one file and then for the other.\n",
    "\n",
    "then it does the merge it on a per file (AKA \"sample\")  basis, merge means doing a linera alignmebt on all the peaks in the bin per sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms2_peaks['prec_bin'] = ms2_peaks.precursor_mz.map(precursors_bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one uses delta res for clustering\n",
    "def bin_mkSurveyLinear_for_ms2(masses, options): # copied from above\n",
    "    # TODO assert masses are ordered\n",
    "    minmass = masses.iloc[0]\n",
    "    up_to = None\n",
    "    for _, mass in masses.iteritems():\n",
    "        if up_to is None:\n",
    "            #up_to = mass + tolerance.getTinDA(mass) this is how its done in some places, but reuslt are not identical to below\n",
    "            deltatol = options[\"MSMSresolution\"].tolerance + (mass - minmass) * options['MSMSresolutionDelta']\n",
    "            up_to = mass + (mass / deltatol)\n",
    "        if mass <= up_to:\n",
    "            yield up_to\n",
    "        else:\n",
    "            deltatol = options[\"MSMSresolution\"].tolerance + (mass - minmass) * options['MSMSresolutionDelta']\n",
    "            up_to = mass + (mass / deltatol)\n",
    "            yield up_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms2_peaks_group_generator(grouped_prec, options):\n",
    "\n",
    "    for idx, prec_ms2_peaks in  grouped_prec:\n",
    "        #intensityWeightedAvg\n",
    "        prec_ms2_peaks.sort_values('mz', inplace=True)\n",
    "        bins1 = list(bin_mkSurveyLinear_for_ms2(prec_ms2_peaks.mz, options))\n",
    "        bins1_weighted_average = (prec_ms2_peaks.mz * prec_ms2_peaks.inty).groupby(bins1).transform('sum') / prec_ms2_peaks.inty.groupby(bins1).transform('sum')\n",
    "        bins2 = list(bin_mkSurveyLinear_for_ms2(bins1_weighted_average, options))\n",
    "        bins2_weighted_average = (prec_ms2_peaks.mz * prec_ms2_peaks.inty).groupby(bins2).transform('sum') / prec_ms2_peaks.inty.groupby(bins2).transform('sum')\n",
    "        bins3 = list(bin_mkSurveyLinear_for_ms2(bins2_weighted_average, options))\n",
    "        weighted_mass = (prec_ms2_peaks.mz * prec_ms2_peaks.inty).groupby(bins3).transform('sum') / prec_ms2_peaks.inty.groupby(bins3).transform('sum')\n",
    "\n",
    "        prec_ms2_peaks['bins'] = bins3\n",
    "        prec_ms2_peaks['weighted_mass'] = weighted_mass\n",
    "\n",
    "        fadi_denominator = prec_ms2_peaks.scan_id.unique().shape[0]\n",
    "        ff_mask = prec_ms2_peaks.groupby(\"bins\")['bins'].transform('count') / fadi_denominator>= options['MSMSfilter']\n",
    "        mof_mask = prec_ms2_peaks.groupby(\"bins\")['bins'].transform('count') / fadi_denominator>= options['MSMSminOccupation']\n",
    "\n",
    "        tf_mask = prec_ms2_peaks.inty > options[\"MSMSthreshold\"]\n",
    "        \n",
    "        # it uses merge sum intensity for getting the averrage intensity...\n",
    "        agg_prec_ms2_peaks = prec_ms2_peaks[ff_mask & tf_mask & mof_mask].groupby('bins').agg({\"weighted_mass\":['mean', 'count'],\"inty\":\"mean\"})\n",
    "        agg_prec_ms2_peaks['precursor_mz'] = prec_ms2_peaks.precursor_mz.mean().round(6) \n",
    "        # there is minor differrence in mean between different files, and the same precursor bin, to avoid it we round\n",
    "        \n",
    "        agg_prec_ms2_peaks['stem'] = idx[0]\n",
    "\n",
    "        agg_prec_ms2_peaks.columns = [\"_\".join(col).strip() for col in agg_prec_ms2_peaks.columns.values]\n",
    "        names = {'weighted_mass_mean':'mz',\n",
    "         'weighted_mass_count':'count', \n",
    "         \"inty_mean\":'inty',\n",
    "         'precursor_mz_':'precursor_mz',\n",
    "         'stem_':'stem'}\n",
    "        agg_prec_ms2_peaks.rename(columns = names, inplace=True)\n",
    "\n",
    "        yield agg_prec_ms2_peaks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms2_peaks.prec_bin.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_prec = ms2_peaks.groupby(['stem','prec_bin'])\n",
    "ms2_agg_peaks = pd.concat(ms2_peaks_group_generator(grouped_prec, options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collape_join_adjecent_clusters_msms(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate ms2 to ms1 scans... \n",
    "ms1_masses = ms1_agg_peaks.mass.drop_duplicates()\n",
    "precur_masses = ms2_agg_peaks.precursor_mz.drop_duplicates()\n",
    "tol = options[\"selectionWindow\"] / 2\n",
    "tmp = pd.merge_asof(ms1_masses, precur_masses.astype(ms1_masses.dtype), left_on='mass', right_on='precursor_mz', direction ='nearest' , tolerance=tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1_agg_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the masterscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:33] {comtypes.client._code_cache:95} INFO - Imported existing <module 'comtypes.gen' from 'c:\\\\Users\\\\mirandaa\\\\Anaconda3\\\\envs\\\\lx128_p3_Dev\\\\lib\\\\site-packages\\\\comtypes\\\\gen\\\\__init__.py'>\n",
      "[12:27:33] {comtypes.client._code_cache:72} INFO - Using writeable comtypes cache directory: 'c:\\Users\\mirandaa\\Anaconda3\\envs\\lx128_p3_Dev\\lib\\site-packages\\comtypes\\gen'\n"
     ]
    }
   ],
   "source": [
    "from lx.spectraContainer import MasterScan\n",
    "from LX2_masterscan import se_factory, ms2entry_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass_inty_generator_ms1_agg(ms1_agg_peaks):\n",
    "    for mass, gdf in ms1_agg_peaks.groupby('mass'):\n",
    "        dictIntensity = gdf.set_index('stem')['lx1_bad_inty'].to_dict()\n",
    "        dictIntensity_lx2 = gdf.set_index('stem')['inty'].to_dict()\n",
    "        dictIntensity.update({ f'{k}_lx2':v for k,v in dictIntensity_lx2.items()})\n",
    "        yield (mass, dictIntensity, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.extend([f'{k}_lx2' for k in samples]) #because we add both results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listSurveyEntry = [\n",
    "        se_factory(msmass, dictIntensity, samples, polarity)\n",
    "        for msmass, dictIntensity, polarity in mass_inty_generator_ms1_agg(\n",
    "            ms1_agg_peaks\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSMSEntry_list_generator(gdf):\n",
    "    for mz,  precur_df in gdf.groupby('mz'):\n",
    "        dictIntensity = precur_df.set_index('stem')['inty'].to_dict()\n",
    "        dictIntensity.update({ f'{k}_lx2':v for k,v in dictIntensity.items()}) #TODO actually get other values\n",
    "        yield (mz, dictIntensity, samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MS2_dict_generator(ms2_agg_peaks):\n",
    "    for precursor_mz, gdf in ms2_agg_peaks.groupby('precursor_mz'):\n",
    "        MSMSEntry_list = [ms2entry_factory(*args) for args in MSMSEntry_list_generator(gdf)]\n",
    "        yield (precursor_mz, MSMSEntry_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MS2_dict = dict(MS2_dict_generator(ms2_agg_peaks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MS2_dict_keys =pd.Series(list(MS2_dict.keys()), name = 'MS2_precurs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MS2_dict_keys.sort_values(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MS1_precurmass =pd.Series([se.precurmass for se in listSurveyEntry], name = 'MS1_precurmass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MS1_precurmass.sort_values(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate ms2 to ms1 scans\n",
    "tol = options[\"selectionWindow\"] / 2\n",
    "precur_map_df = pd.merge_asof(MS1_precurmass, MS2_dict_keys, left_on='MS1_precurmass' , right_on='MS2_precurs',direction ='nearest' , tolerance=tol)\n",
    "# tmp = pd.merge_asof(ms1_masses, ms1_masses.dtype, left_on='mass', right_on='precursor_mz', direction ='nearest' , tolerance=tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precur_dict = precur_map_df.set_index(MS1_precurmass)['MS2_precurs'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(precur_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms1_prec_dict = ms1_agg_peaks.set_index('mass')['precursor_mz'].to_dict()\n",
    "for se in listSurveyEntry:\n",
    "    precursor = precur_dict[se.precurmass]\n",
    "    print(len(MS2_dict.get(precursor,[])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MasterScan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14516/303469480.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# add data to masterscan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMasterScan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mscan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistSurveyEntry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlistSurveyEntry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistSurveyEntry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmassWindow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m  \u001b[1;31m# to avoid bug\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampleOccThr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"MS\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# to avoid bug at def checkOccupation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MasterScan' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# add data to masterscan\n",
    "scan = MasterScan(options)\n",
    "scan.listSurveyEntry = listSurveyEntry\n",
    "scan.listSurveyEntry[0].massWindow = 0.01  # to avoid bug\n",
    "scan.sampleOccThr[\"MS\"] = [(0.0, [])]  # to avoid bug at def checkOccupation\n",
    "scan.sampleOccThr[\"MSMS\"] = [(0.0, [])]\n",
    "\n",
    "# for printing we need\n",
    "# samples.extend([f'{k}_lx2' for k in samples])\n",
    "scan.listSamples = samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tmp_lx1_and_lx2.sc','wb') as f: pickle.dump(scan, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recalibrate\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('ms1_dfs.pkl', 'rb') as f: ms1_dfs = pickle.load(f)\n",
    "with open('options.pkl', 'rb') as f: options = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recalibrate\n",
    "def recalibrate_mzs(mzs, cals):\n",
    "    # from lx2_masterscan\n",
    "    # lx1 takes all that are within tolerance and then uses highest intensity\n",
    "    if not cals or mzs.empty:\n",
    "        return mzs\n",
    "    cal_matchs = [mzs.loc[mzs.sub(cal).abs().idxmin()] for cal in cals]\n",
    "\n",
    "    cal_vals = [cal - cal_match for cal, cal_match in zip(cals, cal_matchs)]\n",
    "    # prefilter\n",
    "    if not any((v < 0.1 for v in cal_vals)):\n",
    "        return mzs\n",
    "    # find near tolerance\n",
    "    cutoff = mzs.diff(-1).quantile(0.1)\n",
    "    is_near = [v < cutoff for v in cal_vals]\n",
    "    if not any(is_near):\n",
    "        log.info(\"no valid calibration masses found\")\n",
    "        return mzs\n",
    "\n",
    "    cal_matchs = [e for e, v in zip(cal_matchs, is_near) if v]\n",
    "    cal_vals = [e for e, v in zip(cal_vals, is_near) if v]\n",
    "    log.debug(\"recalibration info: {'\\n'.join(zip(cal_matchs,cal_vals ))}\")\n",
    "\n",
    "    return mzs + np.interp(mzs, cal_matchs, cal_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for cal_mass in options['MScalibration']:\n",
    "    tol = cal_mass / options[\"MSresolution\"].tolerance\n",
    "    #find close enough most intense\n",
    "    reference_mass = ms1_df[ms1_df.mz.between(cal_mass-tol , cal_mass+tol)].sort_values('inty', ascending=False).mz.iat[0]\n",
    "    change_val = cal_mass - reference_mass\n",
    "    res.append((reference_mass, change_val))\n",
    "\n",
    "cal_matchs, cal_vals = zip(*res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perf testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('stitched.sc','wb') as f: pickle.dump(stitched, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stitched.sc','rb') as f: stitched  = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping Master Scan content\n",
      "containing:\n",
      "8488 MS entries and\n",
      "0 MS/MS entries.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stitched.dump('stitched-dump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Extraction_Blank_1_Ult-s': 339.1315002441406,\n",
       " 'Extraction_Blank_2_Ult-s': 0,\n",
       " 'Ultimate_1-s': 0,\n",
       " 'Ultimate_2-s': 339.1653137207031,\n",
       " 'Ult_05NIST_1-s': 274.6545104980469,\n",
       " 'Ult_05NIST_2-s': 267.81512451171875,\n",
       " 'Ult_10NIST_1-s': 0,\n",
       " 'Ult_10NIST_2-s': 0,\n",
       " 'Ult_1NIST_1-s': 261.4827575683594,\n",
       " 'Ult_1NIST_2-s': 318.67169189453125,\n",
       " 'Ult_5NIST_1-s': 0,\n",
       " 'Ult_5NIST_2-s': 282.861083984375,\n",
       " 'Extraction_Blank_1_Ult-s_lx2': 339.1315002441406,\n",
       " 'Extraction_Blank_2_Ult-s_lx2': 0,\n",
       " 'Ultimate_1-s_lx2': 0,\n",
       " 'Ultimate_2-s_lx2': 339.1653137207031,\n",
       " 'Ult_05NIST_1-s_lx2': 274.6545104980469,\n",
       " 'Ult_05NIST_2-s_lx2': 267.81512451171875,\n",
       " 'Ult_10NIST_1-s_lx2': 0,\n",
       " 'Ult_10NIST_2-s_lx2': 0,\n",
       " 'Ult_1NIST_1-s_lx2': 261.4827575683594,\n",
       " 'Ult_1NIST_2-s_lx2': 318.67169189453125,\n",
       " 'Ult_5NIST_1-s_lx2': 0,\n",
       " 'Ult_5NIST_2-s_lx2': 282.861083984375}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = stitched.listSurveyEntry[0]\n",
    "t.dictIntensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 ('lx128_p3_Dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06b28214ce0e3c48e23cdf5811c1f91197bed88289d9fe34e9aeb8dd4305abb7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
